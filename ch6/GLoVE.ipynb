{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(1000,64)\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "imdb_dir = './imdb'\n",
    "train_dir = os.path.join(imdb_dir, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./imdb\\train\n"
     ]
    }
   ],
   "source": [
    "print(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "texts = []\n",
    "\n",
    "# I had to manually assign encoding = \"utf8\" for it to work\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname),encoding=\"utf8\")\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88582 unique tokens\n"
     ]
    }
   ],
   "source": [
    "#tokenizing the data\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 100\n",
    "training_samples = 200\n",
    "validation_samples = 10000\n",
    "max_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens'% len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  34   44 7576 1414   15    3 4252  514   43   16    3  633  133   12\n",
      "    6    3 1301  459    4 1751  209    3 7693  308    6  676   80   32\n",
      " 2137 1110 3008   31    1  929    4   42 5120  469    9 2665 1751    1\n",
      "  223   55   16   54  828 1318  847  228    9   40   96  122 1484   57\n",
      "  145   36    1  996  141   27  676  122    1  411   59   94 2278  303\n",
      "  772    5    3  837   20    3 1755  646   42  125   71   22  235  101\n",
      "   16   46   49  624   31  702   84  702  378 3493    2 8422   67   27\n",
      "  107 3348]\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor:  (25000, 100)\n",
      "Shape of label tensor:  (25000,)\n",
      "[    0     1     2 ... 24997 24998 24999]\n",
      "[ 3060  8589 15711 ... 11550 21761 19613]\n",
      "[[  14    3 2324 ... 2324  339  155]\n",
      " [ 225    6   50 ...   95   25  250]\n",
      " [9090    2    1 ...    3  159 1537]\n",
      " ...\n",
      " [  11   43   85 ...    6    5   27]\n",
      " [  18  123    1 ...   55    5  213]\n",
      " [5499 5960   31 ...  447 9398    2]]\n",
      "[0 0 1 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor: ', data.shape)\n",
    "print('Shape of label tensor: ', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "print(indices)\n",
    "np.random.shuffle(indices)\n",
    "print(indices)\n",
    "\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "print(data)\n",
    "print(labels)\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
