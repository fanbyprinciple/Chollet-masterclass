{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep convolutional GANs\n",
    "\n",
    "# generator maps latent dims to images of shape (32,32,3)\n",
    "# A discriminatory networkmaps it to binary\n",
    "# a gan network chains the two together maps latent vectors for generator\n",
    "# you train discriminator as any other classification model\n",
    "# to train generator we use the genertaors weights with regard to loss of GAN model\n",
    "\n",
    "# heuristics\n",
    "# we use tanh as final activation\n",
    "# we sample points using normal distribution\n",
    "# Stochasticity s good to induce robustness\n",
    "# Sparse gradients can hinder GAN training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_2:0\", shape=(?, 32), dtype=float32)\n",
      "Tensor(\"dense_2/BiasAdd:0\", shape=(?, 32768), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# GAN generator network\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "\n",
    "latent_dim = 32\n",
    "height = 32\n",
    "width = 32\n",
    "channels = 3\n",
    "\n",
    "generator_input = keras.Input(shape=(latent_dim,))\n",
    "\n",
    "print(generator_input)\n",
    "#First, transform the input into a 16x16 128 channel feature map\n",
    "x = layers.Dense(128 * 16 * 16)(generator_input)\n",
    "print(x)\n",
    "\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Reshape((16,16,128))(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32768)             1081344   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 256)       819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 32, 32, 256)       1048832   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 3)         37635     \n",
      "=================================================================\n",
      "Total params: 6,264,579\n",
      "Trainable params: 6,264,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Then we add a convolutional layer\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "# upsample to 32x32\n",
    "x = layers.Conv2DTranspose(256, 4, strides=2, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "# a few more convolutional layers\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "# 32x32 feature map\n",
    "x = layers.Conv2D(channels, 7, activation='tanh', padding='same')(x)\n",
    "generator = keras.models.Model(generator_input, x)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 30, 30, 128)       3584      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 2, 2, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 790,913\n",
      "Trainable params: 790,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# discriminator model\n",
    "\n",
    "discriminator_input = layers.Input(shape=(height, width, channels))\n",
    "x = layers.Conv2D(128,3)(discriminator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# one dropout layer\n",
    "x = layers.Dropout(0.4)(x)\n",
    "\n",
    "# classification layer\n",
    "x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "discriminator = keras.models.Model(discriminator_input, x)\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0415 06:48:56.493244 14436 deprecation.py:323] From C:\\installs\\Anaconda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# to stabilize training, we use the learning rate decay\n",
    "# and gradient clipping(by value) in the optimizer\n",
    "\n",
    "discriminator_optimizer = keras.optimizers.RMSprop(lr=0.0008, clipvalue=1.0, decay=1e-8)\n",
    "discriminator.compile(optimizer=discriminator_optimizer, loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator needs to be frozen while training gan\n",
    "discriminator.trainable = False\n",
    "\n",
    "gan_input = keras.Input(shape=(latent_dim,))\n",
    "gan_output = discriminator(generator(gan_input))\n",
    "gan = keras.models.Model(gan_input, gan_output)\n",
    "\n",
    "gan_optimizer = keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=1e-8)\n",
    "gan.compile(optimizer=gan_optimizer, loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator loss  0  :  2.570611\n",
      "adverserial loss  0  :  1.1747212e-10\n",
      "discriminator loss  100  :  0.6671761\n",
      "adverserial loss  100  :  0.72095984\n",
      "discriminator loss  200  :  0.69631994\n",
      "adverserial loss  200  :  0.78984845\n",
      "discriminator loss  300  :  0.6973946\n",
      "adverserial loss  300  :  0.6830341\n",
      "discriminator loss  400  :  0.6878629\n",
      "adverserial loss  400  :  0.72458524\n",
      "discriminator loss  500  :  0.69315773\n",
      "adverserial loss  500  :  0.7355903\n",
      "discriminator loss  600  :  0.6852975\n",
      "adverserial loss  600  :  0.72025377\n",
      "discriminator loss  700  :  0.67483103\n",
      "adverserial loss  700  :  0.6131543\n",
      "discriminator loss  800  :  0.69806916\n",
      "adverserial loss  800  :  0.77024883\n",
      "discriminator loss  900  :  0.7076731\n",
      "adverserial loss  900  :  0.80403054\n",
      "discriminator loss  1000  :  0.6961865\n",
      "adverserial loss  1000  :  0.75396615\n",
      "discriminator loss  1100  :  0.70116633\n",
      "adverserial loss  1100  :  0.80903137\n",
      "discriminator loss  1200  :  0.6854866\n",
      "adverserial loss  1200  :  0.7595445\n",
      "discriminator loss  1300  :  0.696231\n",
      "adverserial loss  1300  :  0.75268954\n",
      "discriminator loss  1400  :  0.9988726\n",
      "adverserial loss  1400  :  0.7723673\n",
      "discriminator loss  1500  :  0.79582226\n",
      "adverserial loss  1500  :  0.7855355\n",
      "discriminator loss  1600  :  0.6971178\n",
      "adverserial loss  1600  :  0.73379636\n",
      "discriminator loss  1700  :  0.6954415\n",
      "adverserial loss  1700  :  0.7312738\n",
      "discriminator loss  1800  :  0.75826895\n",
      "adverserial loss  1800  :  0.79650414\n",
      "discriminator loss  1900  :  0.69175184\n",
      "adverserial loss  1900  :  0.74614394\n",
      "discriminator loss  2000  :  0.6990462\n",
      "adverserial loss  2000  :  0.75620544\n",
      "discriminator loss  2100  :  0.694624\n",
      "adverserial loss  2100  :  0.75286704\n",
      "discriminator loss  2200  :  0.69455606\n",
      "adverserial loss  2200  :  0.7796914\n",
      "discriminator loss  2300  :  0.6999845\n",
      "adverserial loss  2300  :  0.7317923\n",
      "discriminator loss  2400  :  0.70329845\n",
      "adverserial loss  2400  :  0.78473127\n",
      "discriminator loss  2500  :  0.6956607\n",
      "adverserial loss  2500  :  0.74100244\n",
      "discriminator loss  2600  :  0.67932206\n",
      "adverserial loss  2600  :  0.7304145\n",
      "discriminator loss  2700  :  0.6854328\n",
      "adverserial loss  2700  :  0.745947\n",
      "discriminator loss  2800  :  0.7153031\n",
      "adverserial loss  2800  :  0.75028443\n",
      "discriminator loss  2900  :  0.70848244\n",
      "adverserial loss  2900  :  0.7508266\n",
      "discriminator loss  3000  :  0.7026134\n",
      "adverserial loss  3000  :  0.7258161\n",
      "discriminator loss  3100  :  0.7005161\n",
      "adverserial loss  3100  :  0.7746761\n",
      "discriminator loss  3200  :  0.69802725\n",
      "adverserial loss  3200  :  0.8300749\n",
      "discriminator loss  3300  :  0.68924874\n",
      "adverserial loss  3300  :  0.7726512\n",
      "discriminator loss  3400  :  0.68290234\n",
      "adverserial loss  3400  :  0.7644184\n",
      "discriminator loss  3500  :  0.68769014\n",
      "adverserial loss  3500  :  0.7510345\n",
      "discriminator loss  3600  :  0.68379533\n",
      "adverserial loss  3600  :  0.7692852\n",
      "discriminator loss  3700  :  0.7382925\n",
      "adverserial loss  3700  :  0.7410455\n"
     ]
    }
   ],
   "source": [
    "# now to train our DCGAN\n",
    "import os\n",
    "from keras.preprocessing import image\n",
    "\n",
    "# Load CIFAR10 data\n",
    "(x_train, y_train), (_,_) =  keras.datasets.cifar10.load_data()\n",
    "\n",
    "# select images class 5\n",
    "x_train = x_train[y_train.flatten()==6]\n",
    "\n",
    "# normalize data\n",
    "x_train = x_train.reshape(\n",
    "        (x_train.shape[0],) + (height, width, channels)).astype('float32')/255.\n",
    "\n",
    "iterations = 10000\n",
    "batch_size = 20\n",
    "save_dir = 'gan_images/'\n",
    "\n",
    "# start training loop\n",
    "start = 0\n",
    "for step in range(iterations):\n",
    "    #sample random points in the latent space \n",
    "    random_latent_vectors = np.random.normal(size=(batch_size,latent_dim))\n",
    "    \n",
    "    # decode them to fake images\n",
    "    generated_images = generator.predict(random_latent_vectors)\n",
    "    \n",
    "    # combine them with real images\n",
    "    stop = start + batch_size\n",
    "    real_images = x_train[start:stop]\n",
    "    combined_images = np.concatenate([generated_images, real_images])\n",
    "    \n",
    "    # Assemble the labels discriminating real from fake images\n",
    "    labels = np.concatenate([np.ones((batch_size,1)),\n",
    "                            np.zeros((batch_size,1))])\n",
    "    \n",
    "    # add random noise to sample\n",
    "    labels += 0.05 * np.random.random(labels.shape)\n",
    "    \n",
    "    # train the discriminator\n",
    "    d_loss = discriminator.train_on_batch(combined_images, labels)\n",
    "    \n",
    "    # sample random points in the latent space\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size,latent_dim))\n",
    "    \n",
    "    # assemble labels that say \"all real images\"\n",
    "    misleading_targets = np.zeros((batch_size,1))\n",
    "    \n",
    "    # train the generator via the gan model where discriminator is frozen\n",
    "    a_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)\n",
    "    \n",
    "    start += batch_size\n",
    "    if(start > len(x_train) - batch_size):\n",
    "        start = 0\n",
    "    \n",
    "    # occasionally save / plot\n",
    "    if step % 100 == 0:\n",
    "        gan.save_weights('gan.h5')\n",
    "        \n",
    "        print(\"discriminator loss \",step, \" : \",d_loss )\n",
    "        print(\"adverserial loss \", step, \" : \", a_loss)\n",
    "        \n",
    "        # save one generated image\n",
    "        img = image.array_to_img(generated_images[0] * 255.,scale=False )\n",
    "        img.save (os.path.join(save_dir, 'generated_five' + str(step) + '.png'))\n",
    "        \n",
    "        #save real image for comparison\n",
    "        img = image.array_to_img(real_images[0] * 255., scale=False)\n",
    "        img.save(os.path.join(save_dir, 'real_five' + str(step) + '.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sample random points on latent space\n",
    "random_latent_vectors = np.random.normal(size=(10,latent_dim))\n",
    "\n",
    "# decode them to fake images\n",
    "generated_images = generator.predict(random_latent_vectors)\n",
    "\n",
    "for i in range(generated_images.shape[0]):\n",
    "    img = image.array_to_img(generated_images[i] * 255., scale=False)\n",
    "    # rescaling\n",
    "    img = rescale(img, (image.shape[0] // 4, image.shape[1] //4), anti_aliasing=True)\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
